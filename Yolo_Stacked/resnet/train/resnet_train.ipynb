{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49576916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x21172d297f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps    \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46bd5af",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac72c6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir dataset\n",
    "!tar -xf ssbi-dataset-v1.0.0.zip -C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87adee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_path = \"dataset/annotations/instances_train.json\"\n",
    "val_json_path = \"dataset/annotations/instances_val.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee9bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a path to a coco json return a df containing filename, category_id, bounding box \n",
    "def coco_to_df(path):\n",
    "    with open(path) as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    #put annotations and images into their own dataframs \n",
    "    annotations = pd.DataFrame(j['annotations'])\n",
    "    images = pd.DataFrame(j['images'])\n",
    "\n",
    "    #merge the two to get all important info and filter for only categories 6 or 7 forged or genuine \n",
    "    df1 = annotations[[\"image_id\", \"category_id\", \"bbox\"]]\n",
    "    df2 = images[[\"id\", \"file_name\"]]\n",
    "    df2 = df2.rename(columns={\"id\": \"image_id\"})\n",
    "    final = df1.merge(df2, on='image_id')\n",
    "\n",
    "    final = final[final['category_id'].isin([6, 7])].reset_index(drop=True)\n",
    "    final['category_id'] = final['category_id'].replace(6, 1)\n",
    "    final['category_id'] = final['category_id'].replace(7, 0)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75feb813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[2574, 856, 284, 307]</td>\n",
       "      <td>check_009_001_F1_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[1671, 599, 166, 180]</td>\n",
       "      <td>check_008_001_F1_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[746, 262, 98, 106]</td>\n",
       "      <td>check_007_001_F1_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[655, 239, 92, 85]</td>\n",
       "      <td>check_004_001_F1_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>[717, 262, 114, 106]</td>\n",
       "      <td>check_007_001_F1_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>4352</td>\n",
       "      <td>0</td>\n",
       "      <td>[818, 331, 109, 70]</td>\n",
       "      <td>check_012_019_G3_6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>4353</td>\n",
       "      <td>0</td>\n",
       "      <td>[1036, 417, 149, 96]</td>\n",
       "      <td>check_016_019_G3_6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>4355</td>\n",
       "      <td>0</td>\n",
       "      <td>[720, 242, 148, 95]</td>\n",
       "      <td>check_020_019_G3_6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>4357</td>\n",
       "      <td>0</td>\n",
       "      <td>[872, 261, 93, 70]</td>\n",
       "      <td>check_017_019_G3_7.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051</th>\n",
       "      <td>4358</td>\n",
       "      <td>0</td>\n",
       "      <td>[651, 235, 84, 63]</td>\n",
       "      <td>check_011_019_G3_7.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3052 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id  category_id                   bbox               file_name\n",
       "0            1            1  [2574, 856, 284, 307]  check_009_001_F1_0.jpg\n",
       "1            2            1  [1671, 599, 166, 180]  check_008_001_F1_0.jpg\n",
       "2            5            1    [746, 262, 98, 106]  check_007_001_F1_0.jpg\n",
       "3            7            1     [655, 239, 92, 85]  check_004_001_F1_1.jpg\n",
       "4            9            1   [717, 262, 114, 106]  check_007_001_F1_1.jpg\n",
       "...        ...          ...                    ...                     ...\n",
       "3047      4352            0    [818, 331, 109, 70]  check_012_019_G3_6.jpg\n",
       "3048      4353            0   [1036, 417, 149, 96]  check_016_019_G3_6.jpg\n",
       "3049      4355            0    [720, 242, 148, 95]  check_020_019_G3_6.jpg\n",
       "3050      4357            0     [872, 261, 93, 70]  check_017_019_G3_7.jpg\n",
       "3051      4358            0     [651, 235, 84, 63]  check_011_019_G3_7.jpg\n",
       "\n",
       "[3052 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[1592, 410, 165, 179]</td>\n",
       "      <td>check_001_001_F1_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[1212, 493, 174, 188]</td>\n",
       "      <td>check_002_001_F1_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[1857, 553, 224, 207]</td>\n",
       "      <td>check_010_001_F1_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>[933, 493, 203, 188]</td>\n",
       "      <td>check_002_001_F1_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[1141, 563, 194, 180]</td>\n",
       "      <td>check_005_001_F1_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>4349</td>\n",
       "      <td>0</td>\n",
       "      <td>[698, 294, 55, 39]</td>\n",
       "      <td>check_019_019_G3_5.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>4353</td>\n",
       "      <td>0</td>\n",
       "      <td>[547, 244, 95, 61]</td>\n",
       "      <td>check_018_019_G3_6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>4355</td>\n",
       "      <td>0</td>\n",
       "      <td>[696, 242, 126, 95]</td>\n",
       "      <td>check_020_019_G3_7.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>4358</td>\n",
       "      <td>0</td>\n",
       "      <td>[747, 331, 93, 70]</td>\n",
       "      <td>check_012_019_G3_7.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>4359</td>\n",
       "      <td>0</td>\n",
       "      <td>[743, 294, 52, 39]</td>\n",
       "      <td>check_019_019_G3_7.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id  category_id                   bbox               file_name\n",
       "0            3            1  [1592, 410, 165, 179]  check_001_001_F1_0.jpg\n",
       "1            4            1  [1212, 493, 174, 188]  check_002_001_F1_0.jpg\n",
       "2            6            1  [1857, 553, 224, 207]  check_010_001_F1_1.jpg\n",
       "3            8            1   [933, 493, 203, 188]  check_002_001_F1_1.jpg\n",
       "4           10            1  [1141, 563, 194, 180]  check_005_001_F1_1.jpg\n",
       "...        ...          ...                    ...                     ...\n",
       "1303      4349            0     [698, 294, 55, 39]  check_019_019_G3_5.jpg\n",
       "1304      4353            0     [547, 244, 95, 61]  check_018_019_G3_6.jpg\n",
       "1305      4355            0    [696, 242, 126, 95]  check_020_019_G3_7.jpg\n",
       "1306      4358            0     [747, 331, 93, 70]  check_012_019_G3_7.jpg\n",
       "1307      4359            0     [743, 294, 52, 39]  check_019_019_G3_7.jpg\n",
       "\n",
       "[1308 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3052"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1308"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = coco_to_df(train_json_path)\n",
    "val_df = coco_to_df(val_json_path)\n",
    "\n",
    "display(train_df)\n",
    "display(val_df)\n",
    "display(len(train_df))\n",
    "display(len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd936c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a dataframe of image and will crop and send back image with id (tensor, id)\n",
    "class SSBI_Dataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "        self.df = df\n",
    "        self.path = path \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        image = Image.open(os.path.join(self.path, row[\"file_name\"])).convert(\"RGB\")\n",
    "\n",
    "        #crop based on bounding box\n",
    "        x, y, w, h = row['bbox']\n",
    "        image = image.crop([x, y, x+w, y+h])\n",
    "\n",
    "        label = int(row[\"category_id\"])\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed169c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    \n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1), \n",
    "    transforms.RandomAffine(\n",
    "        degrees=5, \n",
    "        translate=(0.05, 0.05),\n",
    "        scale=(0.95, 1.05)       \n",
    "    ),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406], \n",
    "        [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e36cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SSBI_Dataset(train_df, \"dataset/train\", train_transform)\n",
    "val_dataset = SSBI_Dataset(val_df, \"dataset/val\", val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34e3d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3052\n",
      "1308\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1895ac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ...,  1.0844,  1.0673,  1.0673],\n",
      "         [-2.1179, -2.1179, -2.1179,  ...,  1.0159,  1.0331, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ...,  1.3782,  1.3606,  1.3256],\n",
      "         [-2.0357, -2.0357, -2.0357,  ...,  1.2731,  1.2906, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ...,  2.5180,  2.5006,  2.4831],\n",
      "         [-1.8044, -1.8044, -1.8044,  ...,  2.4308,  2.4483, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]) 1\n"
     ]
    }
   ],
   "source": [
    "image, label = train_dataset[0]\n",
    "print(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2f706",
   "metadata": {},
   "source": [
    "## Train ResNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf7ac91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {'train' : torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0), \n",
    "               'val' : torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=0)}\n",
    "\n",
    "dataset_sizes = {'train' : len(train_dataset), 'val' : len(val_dataset)}\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ebc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch documentation\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "882127e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch documentation\n",
    "def train_model(model, criterion, optimizer, scheduler, patience=7, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_val_loss = float('inf') \n",
    "    patience_counter = 0 \n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        stop_training_flag = False\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                if phase == 'val':\n",
    "                    \n",
    "                    if epoch_acc > best_acc:\n",
    "                        best_acc = epoch_acc\n",
    "                        torch.save(model.state_dict(), best_model_params_path)\n",
    "                        \n",
    "                    if epoch_loss < best_val_loss:\n",
    "                        best_val_loss = epoch_loss\n",
    "                        patience_counter = 0 \n",
    "                    else:\n",
    "                        patience_counter += 1 \n",
    "\n",
    "                    if patience_counter >= patience:\n",
    "                        print('\\n' + '='*20)\n",
    "                        print(f'EARLY STOPPING: Validation loss did not improve for {patience} epochs.')\n",
    "                        print(f'Stopping at epoch {epoch}.')\n",
    "                        print('='*20)\n",
    "                        stop_training_flag = True\n",
    "                        break \n",
    "\n",
    "            print()\n",
    "            if stop_training_flag:\n",
    "                break\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfcded4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "714da3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.6402 Acc: 0.7274\n",
      "val Loss: 0.4829 Acc: 0.7607\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.6134 Acc: 0.7418\n",
      "val Loss: 0.5019 Acc: 0.7798\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.6942 Acc: 0.7333\n",
      "val Loss: 0.4784 Acc: 0.7821\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.5731 Acc: 0.7507\n",
      "val Loss: 0.6798 Acc: 0.6735\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.5863 Acc: 0.7526\n",
      "val Loss: 0.5717 Acc: 0.7271\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.5824 Acc: 0.7625\n",
      "val Loss: 0.5635 Acc: 0.7202\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.5716 Acc: 0.7556\n",
      "val Loss: 1.0606 Acc: 0.5291\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.4934 Acc: 0.7720\n",
      "val Loss: 0.5880 Acc: 0.7156\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.4460 Acc: 0.7916\n",
      "val Loss: 0.5073 Acc: 0.7546\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.4705 Acc: 0.7798\n",
      "val Loss: 0.5526 Acc: 0.7294\n",
      "\n",
      "====================\n",
      "EARLY STOPPING: Validation loss did not improve for 7 epochs.\n",
      "Stopping at epoch 9.\n",
      "====================\n",
      "\n",
      "Training complete in 12m 38s\n",
      "Best val Acc: 0.782110\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=50)\n",
    "torch.save(model_conv.state_dict(), 'resnet_best_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec6eb6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_path = 'resnet_best_weights.pth'\n",
    "\n",
    "model_conv = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv.load_state_dict(torch.load(best_weights_path, map_location=device, weights_only=False), strict=False)\n",
    "\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model_conv.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#layer4 and last layer\n",
    "optimizer_conv = optim.SGD(\n",
    "    [\n",
    "        {'params': model_conv.fc.parameters(), 'lr': 0.001},  \n",
    "        \n",
    "        {'params': model_conv.layer4.parameters(), 'lr': 0.0001} \n",
    "    ], \n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5557decd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.4495 Acc: 0.8021\n",
      "val Loss: 0.4144 Acc: 0.8012\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.3540 Acc: 0.8470\n",
      "val Loss: 0.5462 Acc: 0.7355\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.2951 Acc: 0.8732\n",
      "val Loss: 0.3753 Acc: 0.8303\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.2520 Acc: 0.8978\n",
      "val Loss: 0.3066 Acc: 0.8769\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.2391 Acc: 0.9050\n",
      "val Loss: 0.4296 Acc: 0.7974\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.2081 Acc: 0.9223\n",
      "val Loss: 0.2408 Acc: 0.9106\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.1951 Acc: 0.9289\n",
      "val Loss: 0.2536 Acc: 0.8998\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.1831 Acc: 0.9348\n",
      "val Loss: 0.2039 Acc: 0.9213\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.1858 Acc: 0.9292\n",
      "val Loss: 0.1542 Acc: 0.9411\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.1834 Acc: 0.9325\n",
      "val Loss: 0.2363 Acc: 0.9037\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.1695 Acc: 0.9384\n",
      "val Loss: 0.2212 Acc: 0.9167\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.1873 Acc: 0.9364\n",
      "val Loss: 0.3342 Acc: 0.8578\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.1659 Acc: 0.9364\n",
      "val Loss: 0.1869 Acc: 0.9320\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.1690 Acc: 0.9371\n",
      "val Loss: 0.2912 Acc: 0.8739\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.1735 Acc: 0.9355\n",
      "val Loss: 0.2082 Acc: 0.9136\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9400\n",
      "val Loss: 0.2549 Acc: 0.8968\n",
      "\n",
      "====================\n",
      "EARLY STOPPING: Validation loss did not improve for 7 epochs.\n",
      "Stopping at epoch 15.\n",
      "====================\n",
      "\n",
      "Training complete in 15m 9s\n",
      "Best val Acc: 0.941131\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=100)\n",
    "torch.save(model_conv.state_dict(), 'resnet_best_weights_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3f6456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_path = 'resnet_best_weights_2.pth'\n",
    "\n",
    "model_conv = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv.load_state_dict(torch.load(best_weights_path, map_location=device, weights_only=False), strict=False)\n",
    "\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model_conv.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model_conv.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#layer4 and last layer\n",
    "optimizer_conv = optim.SGD(\n",
    "    [\n",
    "        {'params': model_conv.fc.parameters(), 'lr': 0.001},  \n",
    "        \n",
    "        {'params': model_conv.layer4.parameters(), 'lr': 0.0001},\n",
    "\n",
    "        {'params': model_conv.layer3.parameters(), 'lr': 0.00001}\n",
    "    ], \n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0921dabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.1807 Acc: 0.9305\n",
      "val Loss: 0.1704 Acc: 0.9320\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.1645 Acc: 0.9440\n",
      "val Loss: 0.1881 Acc: 0.9274\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.1480 Acc: 0.9466\n",
      "val Loss: 0.2060 Acc: 0.9136\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.1406 Acc: 0.9554\n",
      "val Loss: 0.1265 Acc: 0.9495\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.1362 Acc: 0.9554\n",
      "val Loss: 0.2546 Acc: 0.9052\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.1342 Acc: 0.9545\n",
      "val Loss: 0.1662 Acc: 0.9388\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.1298 Acc: 0.9581\n",
      "val Loss: 0.2067 Acc: 0.9174\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.1089 Acc: 0.9663\n",
      "val Loss: 0.1946 Acc: 0.9235\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.1113 Acc: 0.9636\n",
      "val Loss: 0.1648 Acc: 0.9358\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.0984 Acc: 0.9715\n",
      "val Loss: 0.1783 Acc: 0.9289\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.1071 Acc: 0.9699\n",
      "val Loss: 0.1954 Acc: 0.9243\n",
      "\n",
      "====================\n",
      "EARLY STOPPING: Validation loss did not improve for 7 epochs.\n",
      "Stopping at epoch 10.\n",
      "====================\n",
      "\n",
      "Training complete in 14m 1s\n",
      "Best val Acc: 0.949541\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=100)\n",
    "torch.save(model_conv.state_dict(), 'resnet_best_weights_3.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab614fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c62537f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_path = 'resnet_best_weights_3.pth'\n",
    "\n",
    "model_conv = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv.load_state_dict(torch.load(best_weights_path, map_location=device, weights_only=False), strict=False)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_conv = optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce59fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/149\n",
      "----------\n",
      "train Loss: 0.3510 Acc: 0.8834\n",
      "val Loss: 0.0370 Acc: 0.9901\n",
      "\n",
      "Epoch 1/149\n",
      "----------\n",
      "train Loss: 0.0719 Acc: 0.9780\n",
      "val Loss: 0.0213 Acc: 0.9992\n",
      "\n",
      "Epoch 2/149\n",
      "----------\n",
      "train Loss: 0.0388 Acc: 0.9885\n",
      "val Loss: 0.0081 Acc: 0.9992\n",
      "\n",
      "Epoch 3/149\n",
      "----------\n",
      "train Loss: 0.0486 Acc: 0.9882\n",
      "val Loss: 0.0048 Acc: 0.9992\n",
      "\n",
      "Epoch 4/149\n",
      "----------\n",
      "train Loss: 0.0567 Acc: 0.9866\n",
      "val Loss: 0.0018 Acc: 0.9992\n",
      "\n",
      "Epoch 5/149\n",
      "----------\n",
      "train Loss: 0.0198 Acc: 0.9971\n",
      "val Loss: 0.0028 Acc: 0.9992\n",
      "\n",
      "Epoch 6/149\n",
      "----------\n",
      "train Loss: 0.0283 Acc: 0.9961\n",
      "val Loss: 0.0013 Acc: 1.0000\n",
      "\n",
      "Epoch 7/149\n",
      "----------\n",
      "train Loss: 0.0195 Acc: 0.9964\n",
      "val Loss: 0.0023 Acc: 1.0000\n",
      "\n",
      "Epoch 8/149\n",
      "----------\n",
      "train Loss: 0.0214 Acc: 0.9944\n",
      "val Loss: 0.0191 Acc: 0.9954\n",
      "\n",
      "Epoch 9/149\n",
      "----------\n",
      "train Loss: 0.0157 Acc: 0.9954\n",
      "val Loss: 0.0017 Acc: 1.0000\n",
      "\n",
      "Epoch 10/149\n",
      "----------\n",
      "train Loss: 0.0050 Acc: 0.9990\n",
      "val Loss: 0.0010 Acc: 1.0000\n",
      "\n",
      "Epoch 11/149\n",
      "----------\n",
      "train Loss: 0.0088 Acc: 0.9984\n",
      "val Loss: 0.0020 Acc: 1.0000\n",
      "\n",
      "Epoch 12/149\n",
      "----------\n",
      "train Loss: 0.0099 Acc: 0.9974\n",
      "val Loss: 0.0011 Acc: 1.0000\n",
      "\n",
      "Epoch 13/149\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.9987\n",
      "val Loss: 0.0010 Acc: 1.0000\n",
      "\n",
      "Epoch 14/149\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9997\n",
      "val Loss: 0.0017 Acc: 1.0000\n",
      "\n",
      "Epoch 15/149\n",
      "----------\n",
      "train Loss: 0.0089 Acc: 0.9987\n",
      "val Loss: 0.0050 Acc: 0.9985\n",
      "\n",
      "Epoch 16/149\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.9990\n",
      "val Loss: 0.0013 Acc: 0.9992\n",
      "\n",
      "Epoch 17/149\n",
      "----------\n",
      "train Loss: 0.0030 Acc: 0.9993\n",
      "val Loss: 0.0020 Acc: 0.9992\n",
      "\n",
      "====================\n",
      "EARLY STOPPING: Validation loss did not improve for 7 epochs.\n",
      "Stopping at epoch 17.\n",
      "====================\n",
      "\n",
      "Training complete in 27m 57s\n",
      "Best val Acc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=150)\n",
    "torch.save(model_conv.state_dict(), 'resnet_best_weights_4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dcb158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
